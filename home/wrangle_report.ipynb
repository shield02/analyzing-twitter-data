{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were three (3) different sources and three (3) different file types of datasets in this project. The first was dataset was a file on hand. I downloaded this file by simple click of the url link. The file was in a comma seperated value format. I read it into a pandas dataframe using the pandas read_csv format.  The second dataset was a file hosted on a server, so I used a request library with the server's url to make an http request and a response in bite format was sent back which was written into a file format of tsv. These file was then read into a pandas dataframe with correct 'sep' parameters being set in the pandas read_csv function. The third dataset was to gotten from querying the Tweeter API. I created a Tweeter developer account to enable me create a project that will allow me query Tweeter for data of a specific user. To query a Tweeter API, I supplied auth keys from Tweeter to functions in tweepy library to create an object that will be used to query the API. The data returned from the API was in json format, with each tweet as a different value in the json object. The data was then written into a txt file that was then read into a pandas dataframe line by line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I employed both visual and programmatic techniques to assess all the three datasets for quality and tidness issues. In total eleven quality and 2 tidiness issues were documented. In the archived dataset, incorrect data type for three coulmns (tweet_id, timestamp, retweet_status_timestamp) were detected. Tweet records with no images were found. Tweet records that were retweet were found by inspecting the reweet_status_user_id column for no-null records. Inconsistency and inappropriate representation of null values and dog names were found. Was discovered that all dog names starting with small letters were erroneuosly picked by the algorithm that was used to extract the names. Assessing the timestamp column, records beyond August 1st, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this section is to have a tidy dataset to analyze. Following best practices, copies of the original datasets were made before the cleaning process. \n",
    "The tweet_id column of the archived dataframe was changed to an object datatype. The timestamp and retweet_status_timestamp columns were changed to datetime timestamp. \n",
    "Only records with null values were selected in the expanded_urls and tweeter_status_user_id column to handle the problem with tweet records with no images and retweet records respectively.\n",
    "None values in the dog name and dog stages column were replaced with the appropriate NaN value.\n",
    "All the dog names that started with small letters found to be errorneously picked by the algorithm were replaced with the appropraite NaN.\n",
    "For consistency, all values in the text column were changed to be of the same case.\n",
    "Records beyond August 1st, 2017 were removed.\n",
    "tweet_id column of the predictions dataframe was changed to appropriate object datatype.\n",
    "img_num column in predictions dataframe was changed more appropriate category datatype.\n",
    "Duplicate jpg_url records were removed.\n",
    "The values in p1, p2, p3 columns were changed to all lower case and underbars were removed.\n",
    "The tweet_id column datatype was changed to appropriate object datatype.\n",
    "For the tidiness issues:\n",
    "doggo, floofer, pupper, puppo columns were unpivoted into two new columns\n",
    "archived dataframe and the tweet API dataframe were merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
